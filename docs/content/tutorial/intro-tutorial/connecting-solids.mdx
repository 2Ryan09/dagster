---
title: Connecting Solids in Pipelines | Dagster
description: A Dagster pipeline is a set of solids which have data dependencies on each other to create a directed acyclic graph.
---

# Connecting Solids in Pipelines

<CodeReferenceLink filePath="examples/docs_snippets/docs_snippets/intro_tutorial/basics/connecting_solids/" />

Our pipelines wouldn't be very interesting if they were limited to single solids. Pipelines connect solids into arbitrary [DAGs](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of computation.

Splitting up code into solids is similar to splitting up code into functions, with a couple differences:

- Dagster can materialize the output of a solid to persistent storage. [IOManagers](/concepts/io-management/io-managers) give full control over where the outputs go.
- Dagster can execute each solid in a separate process.

These differences mean that we can re-execute some of the solids without needing to re-execute all our code from the beginning. This is useful when dealing with failures in production, as well as when experimenting with a subset of a pipeline in development.

These differences also mean that we can separate business logic from IO. We can use our solids like regular Python functions when we want to test our business logic, and we can use them to build data assets, like data warehouse tables, in production.

Dagster pipelines model a _dataflow_ graph. In data pipelines, the reason that a later step comes after an earlier step is almost always that it uses data produced by the earlier step. Dagster models these dataflow dependencies with _inputs_ and _outputs_.

## Let's Get Serial

We'll expand the pipeline we worked with in the first section of the tutorial into two solids:

- The first solid will download the cereal data and return it as an output.
- The second solid will consume the cereal data produced by the first solid and find the cereal with the most sugar.

```python file=/intro_tutorial/basics/connecting_solids/serial_pipeline.py startafter=start_serial_pipeline_marker_0 endbefore=end_serial_pipeline_marker_0
import requests
import pandas
import io

from dagster import execute_pipeline, pipeline, solid


@solid
def download_cereals(_):
    response = requests.get(
        "https://raw.githubusercontent.com/dagster-io/dagster/master/examples/docs_snippets/docs_snippets/intro_tutorial/cereal.csv"
    )
    return pandas.read_csv(io.BytesIO(response.content))


@solid
def find_sugariest(context, cereals):
    sugariest_index = cereals["sugars"].argmax()
    sugariest_name = cereals["name"][sugariest_index]
    context.log.info(f"{sugariest_name} is the sugariest cereal")


@pipeline
def serial_pipeline():
    find_sugariest(download_cereals())
```

You'll see that we've modified our existing `download_cereals` solid to return an output, in this case the data frame representing the cereals dataset.

We've defined our new solid, `find_sugariest`, to take a user-defined input, `cereals`, in addition to the system-provided <PyObject module="dagster" object="SolidExecutionContext"
displayText="context" /> object.

We can use inputs and outputs to connect solids to each other. Here we tell Dagster that:

- `download_cereals` doesn't depend on the output of any other solid.
- `find_sugariest` depends on the output of `download_cereals`.

Let's visualize this pipeline in Dagit:

```bash
dagit -f serial_pipeline.py
```

Navigate to <http://127.0.0.1:3000/pipeline/serial_pipeline/> or choose "serial_pipeline" from the left sidebar:

<Image
alt="serial_pipeline_figure_one.png"
src="/images/tutorial/serial_pipeline_figure_one.png"
width={1680}
height={946}
/>

<br />

## A More Complex DAG

Solids don't need to be wired together serially. The output of one solid can be consumed by any number of other solids, and the outputs of several different solids can be consumed by a single solid.

```python file=/intro_tutorial/basics/connecting_solids/complex_pipeline.py startafter=start_complex_pipeline_marker_0 endbefore=end_complex_pipeline_marker_0
@solid
def load_cereals(_):
    dataset_path = os.path.join(os.path.dirname(__file__), "cereal.csv")
    with open(dataset_path, "r") as fd:
        cereals = [row for row in csv.DictReader(fd)]
    return cereals


@solid
def sort_by_calories(_, cereals):
    sorted_cereals = list(
        sorted(cereals, key=lambda cereal: cereal["calories"])
    )
    most_calories = sorted_cereals[-1]["name"]
    return most_calories


@solid
def sort_by_protein(_, cereals):
    sorted_cereals = list(
        sorted(cereals, key=lambda cereal: cereal["protein"])
    )
    most_protein = sorted_cereals[-1]["name"]
    return most_protein


@solid
def display_results(context, most_calories, most_protein):
    context.log.info(f"Most caloric cereal: {most_calories}")
    context.log.info(f"Most protein-rich cereal: {most_protein}")


@pipeline
def complex_pipeline():
    cereals = load_cereals()
    display_results(
        most_calories=sort_by_calories(cereals),
        most_protein=sort_by_protein(cereals),
    )
```

First we introduce the intermediate variable `cereals` into our pipeline definition to represent the output of the `load_cereals` solid. Then we make both `sort_by_calories` and `sort_by_protein` consume this output. Their outputs are in turn both consumed by `display_results`.

Let's visualize this pipeline in Dagit:

```bash
dagit -f complex_pipeline.py
```

<Image
alt="complex_pipeline_figure_one.png"
src="/images/tutorial/complex_pipeline_figure_one.png"
width={1680}
height={946}
/>

When you execute this example from Dagit, you'll see that `load_cereals` executes first, followed by `sort_by_calories` and `sort_by_protein`—in any order—and that `display_results` executes last, only after `sort_by_calories` and `sort_by_protein` have both executed.

In more sophisticated execution environments, `sort_by_calories` and `sort_by_protein` could execute not just in any order, but at the same time, since they don't depend on each other's outputs—but both would still have to execute after `load_cereals` (because they depend on its output) and before `display_results` (because `display_results` depends on both of their outputs).

We'll write a simple test for this pipeline showing how we can assert that all four of its solids executed successfully.

```python file=/intro_tutorial/basics/connecting_solids/complex_pipeline.py startafter=start_complex_pipeline_marker_1 endbefore=end_complex_pipeline_marker_1
def test_complex_pipeline():
    res = execute_pipeline(complex_pipeline)
    assert res.success
    assert len(res.solid_result_list) == 4
    for solid_res in res.solid_result_list:
        assert solid_res.success
```

<br />
