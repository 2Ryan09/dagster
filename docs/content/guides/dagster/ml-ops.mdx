---
title: Building your machine learning pipelines In Dagster | Dagster Docs
description: This guide illustrates how to use Dagster to operationaize your machine learning pipeline
---

# Building your machine learning pipelines with Dagster

In this guide, weâ€™ll walk you through how to take your machine learning models and deploy and mantain them in production using Dagster, reliably and efficently.

<Note>
  This guide assumes you have familiarity with machine learning concepts,{" "}
  <a href="/tutorial">the Dagster tutorial</a> and several Dagster concepts,
  including{" "}
  <a href="/concepts/assets/software-defined-assets">software-defined assets</a>{" "}
  and <a href="/concepts/ops-jobs-graphs/jobs">jobs</a> . We also recommend
  reviewing the{" "}
  <a href="/guides/dagster/automating-pipelines">
    automating data pipelines guide
  </a>{" "}
  as many of the same principles can be applied to MLOps.
</Note>

We will work through several aspects of MLOps, including using assets for different elements of your machine learning pipeline, how to automate model training and monitoring your model's drift.

---

## Why build your ML pipelines with Dagster?

- Dagster is designed to use during development, the lightweight execution model means you can access the benefits of an orchestrator, like re-executing from the middle of a pipeline and parallelizing steps while you're experiementing.
- It models data assets, not just tasks so Dagster understands the upstream and downstream data depenedencies and is a one-stop shop for both the data transformations and the models that depend on the data transformations.

---

## Machine learning development

Before we go into how to operatialize a machine learning model, let's go over some of the tools you can use in Dagster to develop a machine learning model.

If you are already using Dagster for your ETL pipelines, it is a natural progression to build out and test your models in Dagster.

For this guide, we will be using the hackernews data which is demoed in the [tutorial](/tutorial).

The machine learning model we will walk through is taking the hackernews stories, using the title to predict the number of comments that a story will generate. This will be a supervised model since we have the number of comments for all the previous stories.

The assets graph will look like this at the end of this guide:

<Image
alt="alt"
src="/images/guides/ml-ops/ml_asset_dag.png"
width={1804}
height={1064}
/>

### Data Ingestion

First, we will be creating an asset which retrieves all the hackernews records from the previous run to the latest hackernews record.

```python file=/guides/dagster/ml_ops/ml_ops.py startafter=data_ingestion_start endbefore=data_ingestion_end
import requests
from dagster import asset, FreshnessPolicy
import pandas as pd

@asset(freshness_policy=FreshnessPolicy(maximum_lag_minutes=10))
def hackernews_stories():
    """Get the max ID number from hacker news"""

    latest_item = requests.get(
        f"https://hacker-news.firebaseio.com/v0/maxitem.json"
    ).json()

    """Get items based on story ids from the HackerNews items endpoint"""
    results = []
    scope = range(latest_item-1000, latest_item)
    for item_id in scope:
        item = requests.get(
            f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        ).json()
        results.append(item)

    df = pd.DataFrame(results)
    if len(df) > 0:
        df = df[df.type == "story"]
        df = df[~df.title.isna()]

    return df
```

### Data Transformation

Now that we have a dataframe with all valid stories, we want to transform that data into something our machine learning model will be able to use.

The first step is taking the dataframe and splitting into a [training and test set](https://en.wikipedia.org/wiki/Training,\_validation,\_and_test_data_sets). In some of your models, you also might choose to have an additional split for a validation set. The reason we split the data is so that we can have a test and/or a validation dataset that is independent of the training set and we can use that to see how well our model did.

```python file=/guides/dagster/ml_ops/ml_ops.py startafter=test_train_split_start endbefore=test_train_split_end
from sklearn.model_selection import train_test_split
from dagster import multi_asset, AssetOut

@multi_asset(outs={'training_data': AssetOut(), 'test_data': AssetOut()})
def training_test_data(hackernews_stories):
    hackernews_stories = hackernews_stories
    X = hackernews_stories.title
    y = hackernews_stories.descendants
    """Split the dataset to reserve 20% of records as the test set"""
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    return (X_train,  y_train), (X_test, y_test)
```

Next, we will take both the training and test data subsets and [tokenize the titles](https://en.wikipedia.org/wiki/Lexical_analysis) e.g. take the words and turn them into columns with the frequency of terms for each records to create [features](https://en.wikipedia.org/wiki/Feature_\(machine_learning\)) for the data. To do this, we will be using the training set to fit the tokenizer, in this case we are using [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and then transforming both the training and test set based with that tokenizer.

```python file=/guides/dagster/ml_ops/ml_ops.py startafter=vectorizer_start endbefore=vectorizer_end
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

@multi_asset(outs={'Tfidf_Vectorizer': AssetOut(), 'transformed_training_data': AssetOut()})
def transformed_train(training_data):
    X_train,  y_train = training_data
    """Initiate and fit the tokenizer on the training data and transform the training dataset"""
    vectorizer = TfidfVectorizer()
    transformed_X_train = vectorizer.fit_transform(X_train)
    transformed_X_train = transformed_X_train.toarray()
    y_train = y_train.fillna(0)
    transformed_y_train = np.array(y_train)

    return vectorizer, (transformed_X_train, transformed_y_train)


@asset
def transformed_test_data(test_data, Tfidf_Vectorizer):
    X_test, y_test = test_data
        """Use the fitted tokenizer to transform the test dataset"""
    transformed_X_test = Tfidf_Vectorizer.transform(X_test)
    transformed_y_test = np.array(y_test)
    y_test = y_test.fillna(0)
    transformed_y_test = np.array(y_test)
    return transformed_X_test, transformed_y_test
```

We have also transformed the dataframes into numpy arrays and removed na values to prepare the data for training.

### Model training

At this point, we have X_train, y_train, X_test, y_test ready to go for our model. We can use any number of models to train our model from libraries like [sklearn](https://scikit-learn.org/), [tensorflow](https://www.tensorflow.org/) and [pytorch](https://pytorch.org/).

In our example, we will be training a [XGBoost model](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor) to predict a numerical value.

```python file=/guides/dagster/ml_ops/ml_ops.py startafter=models_start endbefore=models_end
import xgboost as xg
from sklearn.metrics import mean_absolute_error

@asset
def xgboost(transformed_training_data):
    transformed_X_train, transformed_y_train = transformed_training_data
    """Train XGBoost model, which is a highly efficent and flexible model"""
    xgb_r = xg.XGBRegressor(objective ='reg:squarederror', eval_metric=mean_absolute_error,
                  n_estimators = 20)
    xgb_r.fit(transformed_X_train, transformed_y_train)
    return xgb_r

@asset
def score_xgboost( transformed_test_data, xgboost):
    transformed_X_test, transformed_y_test = transformed_test_data
    """Use the test set data to get a score of the XGBoost model"""
    score = xgboost.score(transformed_X_test, transformed_y_test)
    return score
```

We include freshness policies in the models here which will ensure the upstream data transformation assets are updated to comply with these policies.

### Evaluate our results

In our model assets, we evaluated each of the models on the test data and in this case, got the error between the predicted and actual results. Next, lets create an asset which runs inference on the model, more frequently than the model is re-trained to predict the results.

```python file=/guides/dagster/ml_ops/ml_ops.py startafter=inference_start endbefore=inference_end
@asset(freshness_policy=FreshnessPolicy(maximum_lag_minutes=60))
def model_inference(xgboost, Tfidf_Vectorizer):
    """Get the max ID number from hacker news"""

    latest_item = requests.get(
        f"https://hacker-news.firebaseio.com/v0/maxitem.json"
    ).json()

    """Get items based on story ids from the HackerNews items endpoint"""
    results = []
    scope = range(latest_item-100, latest_item)
    for item_id in scope:
        item = requests.get(
            f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        ).json()
        results.append(item)

    df = pd.DataFrame(results)
    if len(df) > 0:
        df = df[df.type == "story"]
        df = df[~df.title.isna()]
    inference_x = df.title
    inference_x = Tfidf_Vectorizer.transform(inference_x)
    return xgboost.predict(inference_x)
```

Depending on what the objective of your ML model is, you can use this data to set alerts, save model performance history and trigger retraining.

Dagster integrates with [Weights & Biases](/\_apidocs/libraries/dagster-wandb) and an [example](https://github.com/dagster-io/dagster/tree/master/examples/with_wandb) which demonstrates how to use W\&B's artifacts with Dagster.

---

## Machine learning operations

Now that we have set up our data and machine learning pipeline, let's go over some of the Dagster tools to operationalize your pipeline.

### Monitoring

Integrating your machine learning models into your ETL pipelines in Dagster will allow you to understand what assets, including your ML models, were refreshed and when, what has failed. With Dagster, setting up monitoring on the performance on your ML model can not only let you know that something has failed or a model is not performing, but allows you to set up remidation paths such as triggering model retraining that can help automatically resolve issues like model drift.

### Automation

Whether you have a large or small model, Dagster can help automate the data refreshes and model training based on your specific business needs.

If we think through our hackernews example:

- We want to refresh the hackernews stories and store them in a database using [I/O managers](/concepts/io-management/io-managers) every 10 minutes using a [freshness policy](/concepts/partitions-schedules-sensors/asset-sensors#freshness-policy-sensors)
- We want to retrain the machine learning model using a [cron schedule on a daily basis](/concepts/partitions-schedules-sensors/schedules#basic-schedules)
- We want to trigger the hyperparameters to optimize the model when the model performance is declining using a [graph backed asset](/concepts/assets/graph-backed-assets)
- We want to use the model to predict the engagement on some sample titles to see which we should chose to generate maximum engagement on a post. We can set up an input mechanism such as a file in an S3 bucket with a series of titles and a [sensor](/concepts/partitions-schedules-sensors/sensors) that triggers a job that populates the predicted values.

### Asset Partitions

By using asset partitions within the scope of a machine learning pipeline, you can use new partitions of labeled data to continuously track the performance of your model.
