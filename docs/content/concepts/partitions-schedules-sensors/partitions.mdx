---
title: Partitions | Dagster
description: Dagster provides the Partition Set abstraction for pipelines where each run deals with a subset of data.
---

# Partitions

A partitioned job is a job where each run is configured based on a "partition" from a discrete set.

## Relevant APIs

| Name                                         | Description                                                                                         |
| -------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| <PyObject object="Partition" />              | Class defining a logical slice of pipeline configuration.                                           |
| <PyObject object="PartitionsConfig" />       | Class defining a set of <PyObject object="Partition" /> objects.                                    |
| <PyObject object="validate_run_config" />    | A function used to verify that a run config dictionary is valid against a pipeline's config schema. |

## Overview

A partitioned job is a job where each run is configured based on a "partition" from a discrete set. The discrete set is usually a sequence of time windows.  For example, you might write a partitioned job that fills a daily partition of a table each time it runs.

Defining a partitioned job requires defining the set of partitions and defining a function that accepts a partition uses it to generates config for the job.  Having defined a partitioned job, you can kick off a pipeline run or set of pipeline runs by simply selecting partitions in the set.

Partitions have two main uses:

- Partitioned [Schedules](/concepts/partitions-schedules-sensors/schedules): You can construct a schedule that targets a single partition for each run it launches. For example, a pipeline might run each day and process the data that arrived during the previous day.
- [Backfills](/concepts/partitions-schedules-sensors/backfills): You can launch a set of pipeline runs all at once, each run targeting one of the partitions in the set. For example, after making a code change, you might want to re-run your pipeline on every date that it has run on in the past.

---

## Defining a partitioned job

Here's a graph that computes some data for a given date.

```python
@solid(config_schema={"date": str})
def process_data_for_date(context):
    date = context.solid_config["date"]
    context.log.info(f"processing data for {date}")


@solid
def post_slack_message(context):
    context.log.info("posting slack message")


@graph
def my_graph():
    process_data_for_date()
    post_slack_message()
```

The solid `process_data_for_date` takes, as config, a string `date`. This piece of config defines which date to compute data for. For example, if we wanted to compute for May 5th, 2020, we would execute the pipeline with the following config:

```python file=/concepts/partitions_schedules_sensors/config.yaml
solids:
  process_data_for_date:
    config:
      date: "2020-05-05"
```

You can define a <PyObject module="dagster" object="PartitionsConfig"/> object that defines the partitioning and how to define the run config for a given partition. The most common way to do so is to use a decorator like <PyObject module="dagster" object="daily_partitions" decorator/>, which creates a partition for each day since a given start date.

```python
from datetime import datetime

@daily_partitions(start_date="2020-01-01", timezone="US/Pacific")
def my_graph_partitioned_config(start: datetime, end: datetime):
    return {"solids": {"process_data_for_date": {"config": {"date": start.strftime("%Y-%m-%d")}}}}
```

Then, you can construct a partitioned job by supplying the <PyObject module="dagster" object="PartitionsConfig"/> object to the config argument when constructing the job:

```python
my_job = my_graph.to_job(config=my_graph_partitioned_config)
```

## Creating Schedules from Partition Sets

You can construct a schedule that targets a single partition for each run it launches. If you've defined a partitioned job using one of the time-based decorators like <PyObject module="dagster" object="daily_partitions" decorator/>, then you can use <PyObject module="dagster" object="schedule_from_partitions"/>:

```python
my_schedule = schedule_from_partitions(my_job)
```

The created schedule will trigger daily and will launch a run using the partition whose window ends at the trigger time.

You can customize this behavior by changing the `partition_days_offset` parameter for a daily schedule. The default value of this parameter is 1. Setting the value to 0 will cause the schedule to fill a later partition - the one whose window starts at the trigger time. Increasing it above 1 will cause it to fill in an even earlier partition. A similarly-named parameter also exists for the other execution intervals.

## Partitions in Dagit

### The Partitions Tab

In Dagit, you can view runs by partition in the Partitions tab of a Pipeline page.

In the "Run Matrix", each column corresponds to one of the partitions in the partition set. Each row corresponds to one of the steps in the pipeline.

<!-- This was generated with:
    * `dagit -f repo.py` inside docs_snippet/concepts/partitions_schedules_sensors/ directory
    * Navigating to the partitions page for `my_data_pipeline`
-->

<Image
alt="Partitions Tab"
src="/images/concepts/partitions-schedules-sensors/partitions-page.png"
width={3808}
height={2414}
/>

You can click on individual boxes to see the history of runs for that step and partition.

<Image
alt="Partition Step Modal"
src="/images/concepts/partitions-schedules-sensors/partitions-step-modal.png"
width={3808}
height={2414}
/>

### Launching Partitioned Runs from the Playground

You can view and use partitions in the Dagit playground view for a pipeline. In the top bar, you can select from the list of all available partition sets, then choose a specific partition. Within the config editor, the config for the selected partition will be populated.

In the screenshot below, we select the `date_partition_set` and the `2020-05-01` partition, and we can see that the correct run config for the partition has been populated in the editor.

<Image
alt="Partitions in Dagit Playground"
src="/images/concepts/partitions-schedules-sensors/partitions-playground.png"
width={3808}
height={2414}
/>
