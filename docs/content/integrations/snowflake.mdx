---
title: "Using Dagster with Snowflake | Dagster Docs"
description: Store your Dagster assets in Snowflake
---

# Using Dagster with Snowflake

This tutorial focuses on how to store and load Dagster's [software-defined assets (SDAs)](/concepts/assets/software-defined-assets) in Snowflake.

By the end of the tutorial, you will:

- Configure a Snowflake I/O manager
- Create a table in Snowflake using a Dagster asset
- Make a Snowflake table available in Dagster
- Load Snowflake tables in downstream assets

After, you can explore additional Snowflake I/O manager features like:

- Selecting specific columns from Snowflake tables
- Storing assets in multiple Snowflake schema
- Using the Snowflake I/O manager with other I/O managers

---

## Prerequisites

To complete this tutorial, you'll need:

- **To install the `dagster-snowflake` and `dagster-snowflake-pandas` libraries**:

  ```shell
  pip install dagster-snowflake dagster-snowflake-pandas
  ```

- **To gather the following information**, which is required to use the Snowflake I/O manager:

  - **Snowflake account name**: You can find this by logging into Snowflake and getting the account name from the URL:

    <Image
    src="/images/integrations/snowflake/snowflake-account.png"
    width={1456}
    height={72}
    />

  - **Snowflake credentials**: This includes a username and a password. The Snowflake I/O manager can read these values from environment variables. In this guide, we store the username and password as `SNOWFLAKE_USER` and `SNOWFLAKE_PASSWORD`, respectively.

    ```shell
    export SNOWFLAKE_USER=<your username>
    export SNOWFLAKE_PASSWORD=<your password>
    ```

    Refer to the [Using environment variables and secrets guide](/guides/dagster/using-environment-variables-and-secrets) for more info.

---

## Step 1: Configure the Snowflake I/O manager

The Snowflake I/O manager requires some configuration to connect to your Snowflake instance. The `account`, `user` and `password` are required to connect with Snowflake. Additionally, you need to specify a `database` to where all the tables should be stored.

You can also provide some optional configuration to further customize the Snowflake I/O manager. You can specify a `warehouse` and `schema` where data should be stored, and a `role` for the I/O manager.

```python file=/integrations/snowflake/configuration.py startafter=start_example endbefore=end_example
from dagster_snowflake_pandas import snowflake_pandas_io_manager

from dagster import repository, with_resources


@repository
def flowers_analysis_repository():
    return with_resources(
        [iris_dataset],
        resource_defs={
            "io_manager": snowflake_pandas_io_manager.configured(
                {
                    "account": "abc1234.us-east-1",  # required
                    "user": {"env": "SNOWFLAKE_USER"},  # required
                    "password": {"env": "SNOWFLAKE_PASSWORD"},  # required
                    "database": "FLOWERS",  # required
                    "role": "writer",  # optional, defaults to the default role for the account
                    "warehouse": "PLANTS",  # optional, defaults to default warehouse for the account
                    "schema": "IRIS,",  # optional, defaults to PUBLIC
                }
            )
        },
    )
```

With this configuration, if you materialized an asset called `iris_dataset`, the Snowflake I/O manager would be permissioned with the role `writer` and would store the data in the `FLOWERS.IRIS.IRIS_DATASET` table in the `PLANTS` warehouse.

Finally, in the <PyObject object="repository" decorator />, we assign the `snowflake_pandas_io_manager` to the `io_manager` key. `io_manager` is a reserved key to set the default I/O manager for your assets.

For more info about each of the configuration values, refer to the <PyObject module="dagster_snowflake" object="build_snowflake_io_manager" /> API documentation.

---

## Step 2: Create tables in Snowflake

The Snowflake I/O manager can create and update tables for your Dagster defined assets, but you can also make existing Snowflake tables available to Dagster.

<TabGroup>

<TabItem name="Create tables in Snowflake from Dagster assets">

### Store a Dagster asset as a table in Snowflake

To store data in Snowflake using the Snowflake I/O manager, the definitions of your assets don't need to change. You can tell Dagster to use the Snowflake I/O Manager in your repository, like in [Step 1: Configure the Snowflake I/O manager](#step-1-configure-the-snowflake-io-manager), and Dagster will handle storing and loading your assets in Snowflake.

```python file=/integrations/snowflake/basic_example.py
import pandas as pd

from dagster import asset


@asset
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )
```

In this example, we first define our [asset](/concepts/assets/software-defined-assets). Here, we are fetching the Iris dataset as a Pandas DataFrame and renaming the columns. The type signature of the function tells the I/O manager what data type it is working with, so it is important to include the return type `pd.DataFrame`.

When Dagster materializes the `iris_dataset` asset using the configuration from [Step 1: Configure the Snowflake I/O manager](#step-1-configure-the-snowflake-io-manager), the Snowflake I/O manager will create the table `FLOWERS.IRIS.IRIS_DATASET` if it does not exist and replace the contents of the table with the value returned from the `iris_dataset` asset.

</TabItem>

<TabItem name="Make existing tables available in Dagster">

### Make an existing table available in Dagster

You may already have tables in Snowflake that you want to make available to other Dagster assets. You can create [source assets](/concepts/assets/software-defined-assets#defining-external-asset-dependencies) for these tables. By creating a source asset for the existing table, you tell Dagster how to find the table so it can be fetched for downstream assets.

```python file=/integrations/snowflake/source_asset.py
from dagster import SourceAsset

iris_harvest_data = SourceAsset(key="iris_harvest_data")
```

In this example, we create a <PyObject object="SourceAsset" /> for a pre-existing table - perhaps created by an external data ingestion tool - that contains data about iris harvests. To make the data available to other Dagster assets, we need to tell the Snowflake I/O manager how to find the data.

Since we supply the database and the schema in the I/O manager configuration in [Step 1: Configure the Snowflake I/O manager](#step-1-configure-the-snowflake-io-manager), we only need to provide the table name. We do this with the `key` parameter in `SourceAsset`. When the I/O manager needs to load the `iris_harvest_data` in a downstream asset, it will select the data in the `FLOWERS.IRIS.IRIS_HARVEST_DATA` table as a Pandas DataFrame and provide it to the downstream asset.

</TabItem>
</TabGroup>

---

## Step 3: Load Snowflake tables in downstream assets

Once you have created an asset or source asset that represents a table in Snowflake, you will likely want to create additional assets that work with the data. Dagster and the Snowflake I/O manager allow you to load the data stored in Snowflake tables into downstream assets

```python file=/integrations/snowflake/load_downstream.py startafter=start_example endbefore=end_example
import pandas as pd

from dagster import asset

# this example uses the iris_dataset asset from Step 2


@asset
def iris_cleaned(iris_dataset: pd.DataFrame):
    return iris_dataset.dropna().drop_duplicates()
```

In this example, we want to provide the `iris_dataset` asset from the [Store a Dagster asset as a table in Snowflake](#store-a-dagster-asset-as-a-table-in-snowflake) example to the `iris_cleaned` asset.

In `iris_cleaned`, the `iris_dataset` parameter tells Dagster that the value for the `iris_dataset` asset should be provided as input to `iris_cleaned`. If this feels too magical for you, refer to the [docs for explicitly specifying dependencies](/concepts/assets/software-defined-assets#defining-explicit-dependencies).

When materializing these assets, Dagster will use the `snowflake_pandas_io_manager` to fetch the `FLOWERS.IRIS.IRIS_DATASET` as a Pandas DataFrame and pass this DataFrame as the `iris_dataset` parameter to `iris_cleaned`. When `iris_cleaned` returns a Pandas DataFrame, Dagster will use the `snowflake_pandas_io_manager` to store the DataFrame as the `FLOWERS.IRIS.IRIS_CLEANED` table in Snowflake.

---

## Completed code example

When finished, your code should look like the following:

```python file=/integrations/snowflake/full_example.py
import pandas as pd
from dagster_snowflake_pandas import snowflake_pandas_io_manager

from dagster import SourceAsset, asset, repository, with_resources

iris_harvest_data = SourceAsset(key="iris_harvest_data")


@asset
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )


@asset
def iris_cleaned(iris_dataset: pd.DataFrame):
    return iris_dataset.dropna().drop_duplicates()


@repository
def flowers_analysis_repository():
    return with_resources(
        [iris_dataset, iris_harvest_data, iris_cleaned],
        resource_defs={
            "io_manager": snowflake_pandas_io_manager.configured(
                {
                    "account": "abc1234.us-east-1",
                    "user": {"env": "SNOWFLAKE_USER"},
                    "password": {"env": "SNOWFLAKE_PASSWORD"},
                    "database": "FLOWERS",
                    "schema": "IRIS,",
                }
            )
        },
    )
```

---

## Additional features

Now that you're familiar with the base features of the Snowflake I/O manager, let's take a look at some additional features!

---

### Selecting specific columns in a downstream asset

Sometimes you may not want to fetch an entire table as the input to a downstream asset. With the Snowflake I/O manager, you can select specific columns to load by supplying metadata on the downstream asset.

```python file=/integrations/snowflake/downstream_columns.py
import pandas as pd

from dagster import AssetIn, asset

# this example uses the iris_dataset asset from Step 2


@asset(
    ins={
        "iris_sepal": AssetIn(
            key="iris_dataset",
            metadata={"columns": ["Sepal length (cm)", "Sepal width (cm)"]},
        )
    }
)
def sepal_data(iris_sepal: pd.DataFrame) -> pd.DataFrame:
    iris_sepal["Sepal area (cm2)"] = (
        iris_sepal["Sepal length (cm)"] * iris_sepal["Sepal width (cm)"]
    )
    return iris_sepal
```

In this example, we only need the columns containing sepal data in our `sepal_data` asset, so fetching the entire table would be unnecessarily costly. To select specific columns, we need to add metadata to the input asset. We do this in the `metadata` parameter for the `AssetIn` that loads the `iris_dataset` asset. We supply the key `columns` with a list of the column names we want to fetch.

When Dagster materializes `sepal_data` and loads the `iris_dataset` asset using the Snowflake I/O manager, it will only fetch the `Sepal length (cm)` and `Sepal width (cm)` columns of the `FLOWERS.IRIS.IRIS_DATASET` table and pass them to `sepal_data` as a Pandas DataFrame.

---

### Storing tables in multiple schemas

You may want to have different assets stored in different Snowflake schemas. The Snowflake I/O manager allows you to specify the schema in several ways.

If you want all of your assets to be stored in the same schema, you can specify the schema as configuration to the I/O manager, like we did in [Step 1: Configure the Snowflake I/O manager](#step-1-configure-the-snowflake-io-manager).

If you want to store assets in different schemas, you can specify the schema as part of the the asset's asset key:

```python file=/integrations/snowflake/schema.py startafter=start_asset_key endbefore=end_asset_key
import pandas as pd

from dagster import SourceAsset, asset

daffodil_dataset = SourceAsset(key=["daffodil", "daffodil_dataset"])


@asset(key_prefix=["iris"])
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )
```

In this example, the `iris_dataset` asset will be stored in the `IRIS` schema, and the `daffodil_dataset` asset will be found in the `DAFFODIL` schema.

<Note>
  The two options for specifying schema are mutually exclusive. If you provide{" "}
  <code>schema</code> configuration to the I/O manager, you cannot also provide
  it via the asset key and vice versa. If no <code>schema</code> is provided,
  either from configuration or asset keys, the default schema{" "}
  <code>PUBLIC</code> will be used.
</Note>

---

### Using the Snowflake I/O manager with other I/O managers

You may have assets that you don't want to store in Snowflake. You can provide an I/O manager to each asset using the `io_manager_key` parameter in the `asset` decorator:

```python file=/integrations/snowflake/multiple_io_managers.py startafter=start_example endbefore=end_example
import pandas as pd
from dagster_aws.s3.io_manager import s3_pickle_io_manager
from dagster_snowflake_pandas import snowflake_pandas_io_manager

from dagster import asset, repository, with_resources


@asset(io_manager_key="warehouse_io_manager")
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )


@asset(io_manager_key="blob_io_manager")
def iris_plots(iris_dataset):
    return plot_iris_data(iris_dataset)


@repository
def flowers_analysis_repository():
    return with_resources(
        [iris_dataset, iris_plots],
        resource_defs={
            "warehouse_io_manager": snowflake_pandas_io_manager.configured(
                {
                    "database": "FLOWERS",
                    "schema": "IRIS",
                    "account": "abc1234.us-east-1",
                    "user": {"env": "SNOWFLAKE_USER"},
                    "password": {"env": "SNOWFLAKE_PASSWORD"},
                }
            ),
            "blob_io_manager": s3_pickle_io_manager,
        },
    )
```

In this example, the `iris_dataset` asset uses the I/O manager bound to the key `warehouse_io_manager` and `iris_plots` will use the I/O manager bound to the key `blob_io_manager`. In the repository, we supply the I/O managers for those keys. When materialize these assets, the `iris_dataset` will get stored in Snowflake, and `iris_plots` will get saved in S3.

---

## Completed code example

With the additional features, our code example now looks like this:

```python file=/integrations/snowflake/full_example_with_advanced.py startafter=start_full_example endbefore=end_full_example
import pandas as pd
from dagster_aws.s3.io_manager import s3_pickle_io_manager
from dagster_snowflake_pandas import snowflake_pandas_io_manager

from dagster import AssetIn, SourceAsset, asset, repository, with_resources

iris_harvest_data = SourceAsset(key=["iris", "iris_harvest_data"], io_manager_key="warehouse_io_manager")

daffodil_dataset = SourceAsset(key=["daffodil", "daffodil_dataset"], io_manager_key="warehouse_io_manager")


@asset(key_prefix=["iris"], io_manager_key="warehouse_io_manager")
def iris_dataset() -> pd.DataFrame:
    return pd.read_csv(
        "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",
        names=[
            "Sepal length (cm)",
            "Sepal width (cm)",
            "Petal length (cm)",
            "Petal width (cm)",
            "Species",
        ],
    )


@asset(key_prefix=["iris"], io_manager_key="warehouse_io_manager")
def iris_cleaned(iris_dataset: pd.DataFrame):
    return iris_dataset.dropna().drop_duplicates()


@asset(
    ins={
        "iris_sepal": AssetIn(
            key="iris_dataset",
            metadata={"columns": ["Sepal length (cm)", "Sepal width (cm)"]},
        )
    },
    key_prefix=["iris"],
    io_manager_key="warehouse_io_manager"
)
def sepal_data(iris_sepal: pd.DataFrame) -> pd.DataFrame:
    iris_sepal["Sepal area (cm2)"] = (
        iris_sepal["Sepal length (cm)"] * iris_sepal["Sepal width (cm)"]
    )
    return iris_sepal


@asset(io_manager_key="blob_io_manager", key_prefix=["iris"])
def iris_plots(iris_dataset):
    return plot_iris_data(iris_dataset)


@repository
def flowers_analysis_repository():
    return with_resources(
        [
            iris_dataset,
            iris_harvest_data,
            daffodil_dataset,
            iris_cleaned,
            sepal_data,
            iris_plots,
        ],
        resource_defs={
            "warehouse_io_manager": snowflake_pandas_io_manager.configured(
                {
                    "account": "abc1234.us-east-1",
                    "user": {"env": "SNOWFLAKE_USER"},
                    "password": {"env": "SNOWFLAKE_PASSWORD"},
                    "database": "FLOWERS",
                }
            ),
            "blob_io_manager": s3_pickle_io_manager,
        },
    )
```

---

## Related

For more information on software-defined assets, refer to the [Assets tutorial](/tutorial/assets/defining-an-asset) or the [Assets concept documentation](/concepts/assets/software-defined-assets).

For more information on I/O managers, refer to the [I/O manager concept documentation](/concepts/io-management/io-managers).

For more examples of the Snowflake I/O manager in use, see our [Transitioning Data Pipelines from Development to Production](/guides/dagster/transitioning-data-pipeilines-from-development-to-production) and [Testing Against Production with Dagster Cloud Branch Deployments](/guides/dagster/branch_deployments) guides.
