---
title: "Learning Dagster from Airlfow"
description: How to get started with Dagster from an Airflow background
---

# Learning Dagster from Airflow

Airflow users often come to Dagster for a variety of reasons. Some want to take advantage of our great story around testability. Others want to deploy to an orchestrator that is container native and has better support for multiple Python environments. And still others are attracted to the promise of a better abstraction: software-defined assets.

In this tutorial, we'll help you make the switch from Airflow to Dagster. Map the concepts you're familiar with in Airflow to Dagster and provide some examples of the work you did in Airflow can be done in Dagster.

By the end of this tutorial, you will:

- Understand how to author a Dagster Pipeline similiar to a Scheduled Airflow DAG

---

## Comparing an Airflow Dag to Dagster

In this guide we will rewrite an [Airflow DAG](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html) as a Dagster [Job](/concepts/ops-jobs-graphs/jobs). Starting with a basic Airflow DAG:

```python file=/integrations/airflow/dags/tutorial.py
from datetime import datetime, timedelta
from textwrap import dedent

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
with DAG(
    "tutorial",
    # These args will get passed on to each operator
    # You can override them on a per-task basis during operator initialization
    default_args={
        "depends_on_past": False,
        "email": ["airflow@example.com"],
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
    description="A simple tutorial DAG",
    schedule=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=["example"],
) as dag:

    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id="print_date",
        bash_command="date",
    )

    t2 = BashOperator(
        task_id="sleep",
        depends_on_past=False,
        bash_command="sleep 5",
        retries=3,
    )
    t1.doc_md = dedent(
        """\
    #### Task Documentation
    You can document your task using the attributes `doc_md` (markdown),
    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
    rendered in the UI's Task Instance Details page.
    ![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)
    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)
    """
    )

    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR
    dag.doc_md = """
    This is a documentation placed anywhere
    """  # otherwise, type it like this
    templated_command = dedent(
        """
    {% for i in range(5) %}
        echo "{{ ds }}"
        echo "{{ macros.ds_add(ds, 7)}}"
    {% endfor %}
    """
    )

    t3 = BashOperator(
        task_id="templated",
        depends_on_past=False,
        bash_command=templated_command,
    )

    t1 >> [t2, t3]
```

In order to rewrite this DAG in Dagster we will break it down into three parts

1. Define the computations: "the ops" - in Airflow, the operators
2. Define the graph: the job - in Airflow, the DAG
3. Define the schedule - In Airflow, the schedule (how simple!)

A [Job](/concepts/ops-jobs-graphs/jobs) is made up of a [Graph](/concepts/ops-jobs-graphs/graphs) of [Ops](/concepts/ops-jobs-graphs/ops). If you've used the Airflow taskflow api this will feel familiar, With ops you will be focused on writing a graph with python functions as the nodes with the data dependencies between those as edges.

### Step 1: Defining the Ops

After rewriting the operators of the tutorial Airflow Dag as Ops we're left with:

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_ops endbefore=end_ops
@op
def print_date(context) -> datetime:
    ds = datetime.fromisoformat(context.get_tag("date"))
    context.log.info(ds)
    return ds

@op(
    retry_policy=RetryPolicy(
        max_retries=3
    ),
    ins={"start": In(Nothing)}
)
def sleep():
    time.sleep(5)

@op
def templated(context, ds: datetime):
    for i in range(5):
        context.log.info(ds)
        context.log.info(ds - timedelta(days=7))
```

this yeilds us the following graph of ops

<center>
  <Image
    alt="Screenshot of the dagster UI, showing the newly created graph of tutorial Ops"
    src="/images/integrations/airflow/airflow_tutorial_rewrite_ops.png"
    width={2200}
    height={1300}
  />
</center>

Since Dagster is much more data oriented than Airflow you'll notice that we used params to define the execution order, not an arbitrary linking of tasks

##### Op Level Retries

In the tutorial Dag the sleep task allowed for 3 retries, to configure this in dagster you can use [Op level retry policies](/concepts/ops-jobs-graphs/op-retries)

### Step 2: Defining the Job

Now we'll define a Job, which will capture the execution order and data dependencies between our Ops.

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_job endbefore=end_job
@job(tags={"dagster/max_retries": 1, "dag_name": "example"})
def tutorial_job():
    ds = print_date()
    sleep(ds)
    templated(ds)
```

##### Job Level Retries

Job level retries are managed by the [run launcher](/deployment/run-retries) you will need to enable support for them in your dagster.yaml, once you do you can define the retry count on the job.

### Step 3: Defining the schedule

Finally we'll add a schedule

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_schedule endbefore=end_schedule
@schedule(job=tutorial_job, cron_schedule="@daily")
def schedule(context: ScheduleEvaluationContext):
    scheduled_date = context.scheduled_execution_time
    return RunRequest(
        run_key=None,
        tags={"date": scheduled_date.isoformat()},
    )
```
With Dagster you'll use configuration to propigate scheduler specific properties(like run date) to your job runs. There is no templating engine for assembling Dagster Jobs only a schema enforced configuration system.

### Step 4: Running locally

In order to run our newly defined Dagster Job we'll need to define a Dagster [Repository](/concepts/repositories-workspaces/repositories).

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_repo endbefore=end_repo
@repository
def tutorial_repo():
    return [tutorial_job, schedule]
```

We can now load this file with dagit, the interactive web-ui for dagster

```bash
dagit -f <your_dagster_file>.py
```

for reference here's what the file should look like

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_imports endbefore=end_repo
import time
from datetime import datetime, timedelta

from dagster import (
    In,
    Nothing,
    RetryPolicy,
    RunRequest,
    ScheduleEvaluationContext,
    job,
    op,
    repository,
    schedule,
)


@op
def print_date(context) -> datetime:
    ds = datetime.fromisoformat(context.get_tag("date"))
    context.log.info(ds)
    return ds

@op(
    retry_policy=RetryPolicy(
        max_retries=3
    ),
    ins={"start": In(Nothing)}
)
def sleep():
    time.sleep(5)

@op
def templated(context, ds: datetime):
    for i in range(5):
        context.log.info(ds)
        context.log.info(ds - timedelta(days=7))

@job(tags={"dagster/max_retries": 1, "dag_name": "example"})
def tutorial_job():
    dt = print_date()
    sleep(dt)
    templated(dt)

@schedule(job=tutorial_job, cron_schedule="@daily")
def schedule(context: ScheduleEvaluationContext):
    scheduled_date = context.scheduled_execution_time
    return RunRequest(
        run_key=None,
        tags={"date": scheduled_date.isoformat()},
    )

@repository
def rewrite_repo():
    return [tutorial_job, schedule]
```