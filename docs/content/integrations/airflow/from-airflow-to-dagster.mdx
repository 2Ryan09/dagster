---
title: "Learning Dagster from Airlfow"
description: How to get started with Dagster from an Airflow background
---

# Learning Dagster from Airflow

Airflow users often come to Dagster for a variety of reasons. Some want to take advantage of our great story around testability. Others want to deploy to an orchestrator that is container native and has better support for multiple Python environments. And still others are attracted to the promise of a better abstraction: software-defined assets.

In this tutorial, we'll help you make the switch from Airflow to Dagster. Map the concepts you're familiar with in Airflow to Dagster and provide some examples of the work you did in Airflow can be done in Dagster.

By the end of this tutorial, you will:

- Understand how to author a Dagster Pipeline similiar to a Scheduled Airflow DAG

---

## Comparing an Airflow Dag to Dagster

In this guide we will rewrite an [Airflow DAG](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html) as a Dagster [Job](/concepts/ops-jobs-graphs/jobs). Starting with a basic Airflow DAG:

```python file=/integrations/airflow/dags/tutorial.py
from datetime import datetime, timedelta
from textwrap import dedent

from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
    "tutorial",
    default_args={
        "retries": 1,
    },
    description="A simple tutorial DAG",
    schedule=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=["example"],
) as dag:

    t1 = BashOperator(
        task_id="print_date",
        bash_command="date",
    )

    t2 = BashOperator(
        task_id="sleep",
        bash_command="sleep 5",
        retries=3,
    )

    templated_command = dedent(
        """
    {% for i in range(5) %}
        echo "{{ ds }}"
        echo "{{ macros.ds_add(ds, 7)}}"
    {% endfor %}
    """
    )

    t3 = BashOperator(
        task_id="templated",
        bash_command=templated_command,
    )

    t1 >> [t2, t3]
```

In order to rewrite this DAG in Dagster we will break it down into three parts

1. Define the computations: "the ops" - in Airflow, the operators
2. Define the graph: the job - in Airflow, the DAG
3. Define the schedule - In Airflow, the schedule (how simple!)

A [Job](/concepts/ops-jobs-graphs/jobs) is made up of a [Graph](/concepts/ops-jobs-graphs/graphs) of [Ops](/concepts/ops-jobs-graphs/ops). If you've used the Airflow taskflow api this will feel familiar, With ops you will be focused on writing a graph with python functions as the nodes with the data dependencies between those as edges.

### Step 1: Defining the Ops

After rewriting the operators of the tutorial Airflow Dag as Ops we're left with:

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_ops endbefore=end_ops
@op
def print_date(context) -> datetime:
    ds = datetime.now()
    context.log.info(ds)
    return ds


@op(retry_policy=RetryPolicy(max_retries=3), ins={"start": In(Nothing)})
def sleep():
    time.sleep(5)


@op
def templated(context, ds: datetime):
    for _i in range(5):
        context.log.info(ds)
        context.log.info(ds - timedelta(days=7))
```

this yields us the following graph of computations

<center>
  <Image
    alt="Screenshot of the dagster UI, showing the newly created graph of tutorial Ops"
    src="/images/integrations/airflow/airflow_tutorial_rewrite_ops.png"
    width={2200}
    height={1300}
  />
</center>

Since Dagster is much more data oriented than Airflow you'll notice that we used params to define the execution order, not an arbitrary linking of tasks

##### Op Level Retries

In the tutorial Dag the sleep task allowed for 3 retries, to configure this in dagster you can use [Op level retry policies](/concepts/ops-jobs-graphs/op-retries)

### Step 2: Defining the Job

Now we'll define a Job, which will capture the execution order and data dependencies between our Ops.

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_job endbefore=end_job
@job(tags={"dagster/max_retries": 1, "dag_name": "example"})
def tutorial_job():
    ds = print_date()
    sleep(ds)
    templated(ds)
```

##### Job Level Retries

Job level retries are managed by the [run launcher](/deployment/run-retries) you will need to enable support for them in your dagster.yaml, once you do you can define the retry count on the job.

### Step 3: Defining the schedule

Finally we'll add a schedule

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_schedule endbefore=end_schedule
schedule = ScheduleDefinition(job=tutorial_job, cron_schedule="@daily")
```

### Step 4: Running locally

In order to run our newly defined Dagster Job we'll need to define a Dagster [Repository](/concepts/repositories-workspaces/repositories).

```python file=/integrations/airflow/tutorial_rewrite_dagster.py startafter=start_repo endbefore=end_repo
@repository
def rewrite_repo():
    return [tutorial_job, schedule]
```

We can now load this file with dagit, the interactive web-ui for dagster

```bash
dagit -f <your_dagster_file>.py
```

for reference here's what the file should look like

```python file=/integrations/airflow/tutorial_rewrite_complete.py startafter=start_example endbefore=end_example
import time
from datetime import datetime, timedelta

from dagster import (
    In,
    Nothing,
    RetryPolicy,
    ScheduleDefinition,
    job,
    op,
    repository,
    schedule,
)


@op
def print_date(context) -> datetime:
    ds = datetime.now()
    context.log.info(ds)
    return ds


@op(retry_policy=RetryPolicy(max_retries=3), ins={"start": In(Nothing)})
def sleep():
    time.sleep(5)


@op
def templated(context, ds: datetime):
    for _i in range(5):
        context.log.info(ds)
        context.log.info(ds - timedelta(days=7))


@job(tags={"dagster/max_retries": 1, "dag_name": "example"})
def tutorial_job():
    ds = print_date()
    sleep(ds)
    templated(ds)


schedule = ScheduleDefinition(job=tutorial_job, cron_schedule="@daily")


@repository
def rewrite_repo():
    return [tutorial_job, schedule]
```
